# ü§ñ SENTERANGA Hybrid RAG Server (Jokko)

Ce projet h√©berge le serveur backend pour l'assistant intelligent **Jokko** de la plateforme SENTERANGA. Il utilise une architecture **RAG (Retrieval-Augmented Generation)** hybride :
1.  **Recherche S√©mantique Locale** : Utilisation de FAISS et SentenceTransformers pour retrouver instantan√©ment les documents pertinents du projet.
2.  **G√©n√©ration Cloud** : Utilisation de **Google Gemini 1.5 Flash** pour g√©n√©rer des r√©ponses fluides et intelligentes en fran√ßais.

## üåü Fonctionnalit√©s

*   **R√©ponses Contextuelles** : L'IA lit la documentation du projet pour r√©pondre aux questions sp√©cifiques.
*   **Performance** : Recherche locale instantan√©e (<100ms) et g√©n√©ration rapide via Gemini.
*   **D√©ploiement Facile** : Configur√© pour Render, sans besoin de GPU co√ªteux.
*   **API Simple** : Endpoints REST clairs pour l'int√©gration avec Angular.

## üèóÔ∏è Architecture Technique

```mermaid
graph TD
    A[Frontend Angular] -->|POST /chat| B(FastAPI Server)
    B -->|1. Embed Query| C[Sentence Transformer]
    C -->|Vector| D[FAISS Vector DB]
    D -->|2. Retrieve Docs| B
    B -->|3. Send Context + Query| E[Google Gemini API]
    E -->|4. Answer| B
    B -->|Response| A
```

## üìã Pr√©requis

*   **Python 3.10+** install√©.
*   **Cl√© API Google Gemini** (gratuite). [Obtenir une cl√© ici](https://aistudio.google.com/app/apikey).
*   Un compte **Render** (pour le d√©ploiement).

## üöÄ Installation Locale

### 1. Cloner et Pr√©parer

```bash
# Cr√©er un environnement virtuel
python3 -m venv .venv

# Activer l'environnement
source .venv/bin/activate  # Sur Windows: .venv\Scripts\activate

# Installer les d√©pendances
pip install -r requirements.txt
```

### 2. Configurer l'API Key

Vous devez d√©finir la variable d'environnement `GEMINI_API_KEY`.

```bash
# Linux/Mac
export GEMINI_API_KEY="votre_cl√©_api_ici"

# Windows (Powershell)
$env:GEMINI_API_KEY="votre_cl√©_api_ici"
```

### 3. Indexer le Corpus de Documents

Cette √©tape cr√©e la "m√©moire" de l'IA en scannant les fichiers du projet.

```bash
# Indexer le dossier courant (ou sp√©cifiez un chemin)
python index_corpus.py .
```
*Cela va cr√©er un dossier `index_data/` contenant l'index vectoriel.*

### 4. Lancer le Serveur

```bash
python -m uvicorn server:app --host 127.0.0.1 --port 8000 --reload
```

Le serveur sera accessible sur `http://127.0.0.1:8000`.

## üì° API Endpoints

### `GET /health`
V√©rifie l'√©tat du serveur et de l'index.
```json
{
  "status": "OK",
  "index_loaded": true,
  "gemini_configured": true
}
```

### `POST /chat`
Envoyer un message √† Jokko.
**Corps de la requ√™te :**
```json
{
  "message": "Comment d√©ployer le projet ?",
  "userContext": {}
}
```
**R√©ponse :**
```json
{
  "response": "Pour d√©ployer le projet, suivez les √©tapes...",
  "sources": ["README.md", "README_DEPLOY.md"],
  "backend": "gemini"
}
```

## ‚òÅÔ∏è D√©ploiement sur Render

Pour mettre ce serveur en ligne gratuitement :

1.  **Pousser le code** sur un d√©p√¥t GitHub/GitLab.
    *   *Assurez-vous d'inclure le dossier `index_data/` g√©n√©r√© dans votre commit* (sinon le serveur ne saura rien).
2.  Cr√©er un **Web Service** sur [Render.com](https://render.com).
3.  Connecter votre d√©p√¥t.
4.  **Configuration** :
    *   **Runtime**: Python 3
    *   **Build Command**: `pip install -r requirements.txt`
    *   **Start Command**: `python -m uvicorn server:app --host 0.0.0.0 --port $PORT`
5.  **Environment Variables** :
    *   Ajouter `GEMINI_API_KEY` avec votre cl√©.

## üìÇ Structure du Projet

*   `server.py` : Le c≈ìur de l'application FastAPI.
*   `index_corpus.py` : Script pour transformer vos fichiers en vecteurs FAISS.
*   `index_data/` : Base de donn√©es vectorielle (ne pas supprimer si vous d√©ployez).
*   `requirements.txt` : Liste des librairies Python n√©cessaires.

## üõ°Ô∏è Alternative : Mode 100% Hors Ligne

Si vous ne souhaitez pas utiliser l'API Gemini et pr√©f√©rez une ex√©cution locale (avec GPT4All ou LlamaCPP), utilisez le script `server_faiss_only.py`.

1.  **Lancer le serveur alternatif** :
    ```bash
    # Activer le t√©l√©chargement auto de mod√®les (GPT4All)
    export ENABLE_GPT4ALL_DOWNLOAD=1
    python -m uvicorn server_faiss_only:app --host 127.0.0.1 --port 8000
    ```
    *Note: Cela t√©l√©chargera un mod√®le (~4GB) au premier lancement.*

## üß™ Tests

Un script de test est disponible pour v√©rifier que tout fonctionne avant d√©ploiement :
```bash
python test_deploy.py
```

---
**Note**: Si vous modifiez la documentation ou le code source, pensez √† relancer `python index_corpus.py .` pour mettre √† jour les connaissances de l'IA.
